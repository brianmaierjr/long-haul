---
layout: article
title: "Deep Learning (5/5): Sequence Models"
intro: | 
    The last course introduces various forms of NN-Models that take their input as a sequence of tokens. Starting with Recurrent Neural Networks (RNN) for speech/text processing you get to know word embeddings as a special form of Natural Language Processing (NLP). Finally, you learn about Sequence-to-Sequence Models that take a sequence as an input and also produce a sequence as an output. In the first week’s assignment you will implement two generative models: a RNN that can generate music that sounds like improvized Jazz. You also implement another form of an RNN that deals with textual data which can generate random names for dinosaurs. In the second week you will implement some core functions of NLP models such as calculating the similarity between two words or removing the gender bias. You will also implement a RNN that can classify an arbitrary text with a suitable Emoji. Finally you will put your newly learned knowledge about Attention Models into practice by implementing some functions of an RNN that can be used for machine translation. You will also learn how to implement a model that is able to detect trigger words from audio clips.
permalink: /ml/deep-learning/4
---

| **Week**                   | **Content**                   | **Introduced Concepts** |
| 1                   | In the first week you know Recurrent Neural Networks (RNN) as a special form of NN and what types of problems they’re good at. You also learn why a traditional NN is not suitable for these kinds of problems.                   | Recurrent Neural Network (RNN)<br>Sequence Tokens<br>Many-to-Many/Many-to-One/One-To-Many-Architectures<br>Gated Recurrent Unit (GRU)<br>Long Short Term Unit (LSTM)<br>Bidirectional RNN (BRNN)<br>Deep-RNN |
| 2                   | The second week is all about NLP. You learn how word embeddings can help you with NLP tasks and how you can deal with bias.                   | Word Embeddings<br>t-SNE<br>Word2Vec & GloVe<br>Cosinus-Similarity<br>One-Hot-Encoding<br>Skip-Gram and CBOW<br>Negative Sampling<br>Context & Target-Wort<br>Sentiment Classification<br>Debiasing |
| 3                   | The last and final week of this specialization introduces the concept of Attention Models as a special form of Sequence-to-Sequence models and how they can be used for machine translation.                   | Sequence-to-Sequence Models<br>Encoder/Decoder-Networks<br>Conditional Language Models<br>Attention Models<br>Greedy vs. Beam search<br>Length Normalization<br>Bleu Score<br>Connectionist Temoral Classification (CTC)<br>Trigger Word Dettection |