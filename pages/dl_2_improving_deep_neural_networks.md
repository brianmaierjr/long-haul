---
layout: article
title: "Deep Learning (2/5): Improving Deep Neural Networks"
intro: | 
    This course focuses on common problems you might encounter when working on your own DL-projects. The course introduces some useful recipes that might help you when your algorithm is not performing as well as it should. It also introduces best practices when training your own NN and some useful techniques. You will deepen your understanding of how to optimize your hyperparameters and convergence speed. In the last week you get into touch with a DL-Framework (TensorFlow) for the first time.
permalink: /ml/deep-learning/2
previous:
    url: /ml/deep-learning/1
    title: Part 1: Neural Networks and Deep Learning
next: 
    url: /ml/deep-learning/3
    title: Part 3: Structuring ML projects
---

| **Week**                   | **Content**                   | **Introduced Concepts** |
| 1                   | Week 1 focuses on the various hyperparameters of a model. You also learn how to recognize problems with your algorithm and where they are rooted.                   | probability distributions<br>bias and variance<br>over- and underfitting<br>regularisation techniques (L2, Dropout, Early Stopping)<br>data augmentation<br>input normalization<br>weight decay<br>exploding/vanishing gradients |
| 2                   | Week 2 introduces some optimization algorithms that can speed up the overall learning process.                   | Mini-Batch Gradient Descent<br>Exponentially Weighted Average<br>Momentum<br>RMSprop<br>Adam<br>Learning Rate Decay<br>Local optima |
| 3                   | Week 3 wraps up on hyperparameters and how to find optimal values for them. It also introduces Softmax as an alternative activation function for multiclass-classification.                   | Pandas & Caviar<br>Hyperparameter Tuning<br>Batch Norm<br>TensorFlow |
